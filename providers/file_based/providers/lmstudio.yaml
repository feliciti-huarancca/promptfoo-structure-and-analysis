# Local model
id: openai:chat                                 # generic chat endpoint
config:
  apiBaseUrl: http://127.0.0.1:1234/v1
  apiKey: lmstudio                              # any string works
  model: llama-3.2-1b-instruct                  # from GET /v1/models
  temperature: 0.7
  max_tokens: 300